{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d5609b",
   "metadata": {},
   "source": [
    "# Week 1 Practical Aspect of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b7fe7",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15478086",
   "metadata": {},
   "source": [
    "If your Neural Network model seems to have high bias, what of the following would be promising things to try? (Check all that apply.)\n",
    "\n",
    "Correct:\n",
    "1. Increasing numbers of units in each hidden layer\n",
    "2. Make the neuron network deeper\n",
    "\n",
    "Wrong:\n",
    "Get more training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938dbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6b5d4e1",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ccdf101",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'public_tests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rd/q2lzfdss4b167026m7l6h8pm0000gn/T/ipykernel_33121/2621888313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpublic_tests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minit_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minit_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_decision_boundary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_dec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'public_tests'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from public_tests import *\n",
    "from init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\n",
    "from init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# load image dataset: blue/red dots in circles\n",
    "# train_X, train_Y, test_X, test_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97a5c1",
   "metadata": {},
   "source": [
    "### initialize_parameters_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6507086",
   "metadata": {},
   "source": [
    "    for l in range(1, L):\n",
    "    # YOUR CODE STARTS HERE\n",
    "    parameters['W' + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1]))\n",
    "    parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n",
    "    # YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce02aa0",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember**:\n",
    "- The weights $W^{[l]}$ should be initialized randomly to break symmetry. \n",
    "- However, it's okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34a64d",
   "metadata": {},
   "source": [
    "### Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e807fb",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**In summary**:\n",
    "- Initializing weights to very large random values doesn't work well. \n",
    "- Initializing with small random values should do better. The important question is, how small should be these random values be? Let's find out up next!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9edc3",
   "metadata": {},
   "source": [
    "**Optional Read:**\n",
    "\n",
    "\n",
    "The main difference between Gaussian variable (`numpy.random.randn()`) and uniform random variable is the distribution of the generated random numbers:\n",
    "\n",
    "- numpy.random.rand() produces numbers in a [uniform distribution](https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/rand.jpg).\n",
    "- and numpy.random.randn() produces numbers in a [normal distribution](https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/randn.jpg).\n",
    "\n",
    "When used for weight initialization, randn() helps most the weights to Avoid being close to the extremes, allocating most of them in the center of the range.\n",
    "\n",
    "An intuitive way to see it is, for example, if you take the [sigmoid() activation function](https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/sigmoid.jpg).\n",
    "\n",
    "You’ll remember that the slope near 0 or near 1 is extremely small, so the weights near those extremes will converge much more slowly to the solution, and having most of them near the center will speed the convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43379f32",
   "metadata": {},
   "source": [
    "### initialize_parameters_he"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb70615",
   "metadata": {},
   "source": [
    "Implement the following function to initialize your parameters with He initialization. This function is similar to the previous `initialize_parameters_random(...)`. The only difference is that instead of multiplying `np.random.randn(..,..)` by 10, you will multiply it by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d05f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2./layers_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469a0ea",
   "metadata": {},
   "source": [
    "You've tried three different types of initializations. For the same number of iterations and same hyperparameters, the comparison is:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>Model</b>\n",
    "        </td>\n",
    "        <td>\n",
    "            <b>Train accuracy</b>\n",
    "        </td>\n",
    "        <td>\n",
    "            <b>Problem/Comment</b>\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        3-layer NN with zeros initialization\n",
    "        </td>\n",
    "        <td>\n",
    "        50%\n",
    "        </td>\n",
    "        <td>\n",
    "        fails to break symmetry\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with large random initialization\n",
    "        </td>\n",
    "        <td>\n",
    "        83%\n",
    "        </td>\n",
    "        <td>\n",
    "        too large weights \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with He initialization\n",
    "        </td>\n",
    "        <td>\n",
    "        99%\n",
    "        </td>\n",
    "        <td>\n",
    "        recommended method\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edcff75",
   "metadata": {},
   "source": [
    "**Congratulations**! You've completed this notebook on Initialization. \n",
    "\n",
    "Here's a quick recap of the main takeaways:\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "- Different initializations lead to very different results\n",
    "- Random initialization is used to break symmetry and make sure different hidden units can learn different things\n",
    "- Resist initializing to values that are too large!\n",
    "- He initialization works well for networks with ReLU activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0333f00",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3801a889",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reg_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rd/q2lzfdss4b167026m7l6h8pm0000gn/T/ipykernel_38425/1127406382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreg_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_decision_boundary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_2D_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_dec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreg_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtestCases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'reg_utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy.io\n",
    "from reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\n",
    "from reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\n",
    "from testCases import *\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c8ceb",
   "metadata": {},
   "source": [
    "### Non-regularized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- If True, print the cost every 10000 iterations\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = []                            # to keep track of the cost\n",
    "    m = X.shape[1]                        # number of examples\n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        if keep_prob == 1:\n",
    "            a3, cache = forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "        # Cost function\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(a3, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout, \n",
    "                                                # but this assignment will only explore one at a time\n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 10000 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (x1,000)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cc4e9",
   "metadata": {},
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a95d6",
   "metadata": {},
   "source": [
    "#### compute_cost_with_regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee83ea",
   "metadata": {},
   "source": [
    "Implement `compute_cost_with_regularization()` which computes the cost given by formula (2). To calculate $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$  , use :\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "```\n",
    "Note that you have to do this for $W^{[1]}$, $W^{[2]}$ and $W^{[3]}$, then sum the three terms and multiply by $ \\frac{1}{m} \\frac{\\lambda}{2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ee311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n",
    "    \n",
    "    #(≈ 1 lines of code)\n",
    "    # L2_regularization_cost = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    L2_regularization_cost = lambd*(np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))/(2*m)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f3bc9",
   "metadata": {},
   "source": [
    "#### backward_propagation_with_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd/m * W3\n",
    "    # YOUR CODE ENDS HERE\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd/m * W2\n",
    "    # YOUR CODE ENDS HERE\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T) + lambd/m *W1\n",
    "    # YOUR CODE ENDS HERE\n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a29fda",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The value of $\\lambda$ is a hyperparameter that you can tune using a dev set.\n",
    "- L2 regularization makes your decision boundary smoother. If $\\lambda$ is too large, it is also possible to \"oversmooth\", resulting in a model with high bias.\n",
    "\n",
    "**What is L2-regularization actually doing?**:\n",
    "\n",
    "L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. \n",
    "\n",
    "<br>\n",
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember:** the implications of L2-regularization on:\n",
    "- The cost computation:\n",
    "    - A regularization term is added to the cost.\n",
    "- The backpropagation function:\n",
    "    - There are extra terms in the gradients with respect to weight matrices.\n",
    "- Weights end up smaller (\"weight decay\"): \n",
    "    - Weights are pushed to smaller values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0be00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
